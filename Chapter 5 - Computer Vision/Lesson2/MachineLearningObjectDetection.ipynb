{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection with Synthetic Data using Blender and YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "In this lesson, we will explore how to generate synthetic image data using **Blender**, an open-source 3D creation suite, and use it to train an object detection model with **YOLO (You Only Look Once)**.\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand how to automate Blender for dataset generation.\n",
    "- Learn how to create annotations in YOLO format.\n",
    "- Convert images to grayscale to force the model to relay less on the object's color.\n",
    "- Train and test a YOLOv11 object detection model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What We'll Do\n",
    "We’ll use **Blender** to create synthetic datasets for object detection tasks in machine learning.  \n",
    "Synthetic data allows us to generate large, labeled datasets without the time-consuming process of manual image collection and annotation.  \n",
    "\n",
    "For the detection model, we’ll use the **YOLO Python library**, which provides tools for both training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up Blender and Python Environment\n",
    "To begin, ensure you have Blender installed on your system. You can download it from [blender.org](https://www.blender.org/download/).\n",
    "\n",
    "Download or create your own HDRI's (environment textures) to use inside Blender for random backgrounds.\n",
    "[Download Free, CC0, High Resolution HDRI's](https://polyhaven.com/hdris) (Use HDR format)\n",
    "\n",
    "Additionally, set up your Python environment with necessary libraries such as `cv2`, `ultralytics`.\n",
    "\n",
    "\n",
    "Finlay If you have a dedicated GPU in your system, make sure to set up Blender and pytorch to use it for rendering and model training.\n",
    "\n",
    "Use the cell below to check your environment and make sure all the packages are working properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Note: Run this in a Jupyter environment with internet access\n",
    "\n",
    "# YOLOv11 (Ultralytics)\n",
    "%pip install ultralytics --quiet\n",
    "\n",
    "# OpenCV for image processing\n",
    "%pip install opencv-python --quiet\n",
    "\n",
    "\n",
    "# Install torch with CUDA support if a compatible GPU is available\n",
    "# https://pytorch.org/get-started/locally/\n",
    "\n",
    "\n",
    "# Blender's Python API is included in Blender itself.\n",
    "# If you want to run Blender scripts from outside Blender, you need to call Blender in background mode:\n",
    "# blender --background --python your_script.py\n",
    "\n",
    "# Check versions\n",
    "import sys\n",
    "import cv2\n",
    "import ultralytics\n",
    "import os\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"OpenCV version:\", cv2.__version__)\n",
    "print(\"Ultralytics YOLO version:\", ultralytics.__version__)\n",
    "print(\"PyTorch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available for PyTorch training\n",
    "# This is important for performance, especially with large models and datasets\n",
    "# For more information, visit: https://pytorch.org/docs/stable/notes/cuda.html\n",
    "# If you encounter issues, ensure that your CUDA drivers are correctly installed and compatible with your PyTorch version.\n",
    "print(\"Is GPU available for PyTorch:\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Automating Blender for Synthetic Data Generation\n",
    "\n",
    "In this section, we will use **Blender's Python API** to automate the rendering of synthetic images.  \n",
    "This includes randomizing environments, objects, and camera angles to generate diverse training data for machine learning applications.\n",
    "<br>\n",
    "\n",
    "### ⚠️ <span style=\"color:red\">Important Notice</span>\n",
    "\n",
    "Before running the automation script and rendering the animation, make sure the following settings are correctly configured in Blender:\n",
    "\n",
    "- **Render Engine:** Set to `Cycles`\n",
    "- **Device:** Set to `GPU Compute`  \n",
    "  *(If it it is gray go to: `Edit` → `Preferences` → `System` → `Cycles Render Devices`)*\n",
    "\n",
    "<img src=\"attachments/System.png\" alt=\"System Preferences\" width=\"650\"/>\n",
    "<img src=\"attachments/RenderPreferences.png\" alt=\"Render Preferences\" width=\"480\"/>\n",
    "\n",
    "- **Output Settings:**\n",
    "  - Choose the correct **output folder**\n",
    "  - The **Frame Range** defines how much images you will get\n",
    "  - Set **file format** to `JPEG`\n",
    "\n",
    "<img src=\"attachments/OutputPreferences.png\" alt=\"Output Preferences\" width=\"400\"/>\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **<span style=\"color:green\">Run the script and Render the animation</span>**\n",
    "The cell below contains a basic Blender automation script to get you started, to run it navigate to the `Scripting` tab and press *Run Script*.\n",
    "\n",
    "After running the script, scroll through the timeline at the bottom and check that the camera is moving as expected.\n",
    "\n",
    "Finally press `Render` → `Render Animation`.\n",
    "\n",
    "Be sure to configure your scene as described above before rendering.\n",
    "\n",
    "<img src=\"attachments/RunScript.png\" alt=\"Output Preferences\" width=\"1000\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bpy\n",
    "import mathutils\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "from math import pi\n",
    "\n",
    "# -------- CONFIGURATION --------\n",
    "# Camera settings\n",
    "# Camera x position range\n",
    "radius_range = (1, 10)  # in Blender units (meters)\n",
    "\n",
    "# Camera y position range\n",
    "theta_range = (0, 2 * pi)  # in Blender units (radians)\n",
    "\n",
    "# Camera z position range\n",
    "phi_range = (10 * (pi / 180), pi / 2)  # in Blender units (radians)\n",
    "\n",
    "# Camera x rotation offset range (in radians)\n",
    "x_rotation_offset_range = (-6 / (180 / pi), 6 / (180 / pi))  # in radians\n",
    "\n",
    "# Camera z rotation offset range (in radians)\n",
    "z_rotation_offset_range = (-18 / (180 / pi), 18 / (180 / pi))  # in radians\n",
    "\n",
    "# Name of the camera object in the scene\n",
    "camera_name = \"Camera\"\n",
    "\n",
    "# Name of the target object the camera should point at\n",
    "target_object_name = \"FRC_Coral\"\n",
    "\n",
    "# Environment texture settings\n",
    "# Folder containing .hdr files\n",
    "hdr_folder = \"Path/HDRIs/8K_HDRI\"\n",
    "\n",
    "# Name of the object whose material has the image texture\n",
    "object_name = \"Sphere\"\n",
    "\n",
    "# Name of the image texture node in the object's material\n",
    "object_texture_node_name = \"Image Texture\"\n",
    "\n",
    "# Name of the environment texture node in the world\n",
    "world_texture_node_name = \"Environment Texture\"\n",
    "\n",
    "# How often to change the environment texture (in frames)\n",
    "change_interval = 10\n",
    "# --------------------------------\n",
    "\n",
    "#region checks\n",
    "# -------- CHECKS --------\n",
    "# Check that the Blender scene is configured correctly\n",
    "\n",
    "# Get list of all .hdr files\n",
    "hdr_files = [f for f in os.listdir(hdr_folder) if f.lower().endswith(\".hdr\")]\n",
    "hdr_files.sort()  # ensure consistent order\n",
    "\n",
    "if not hdr_files:\n",
    "    raise FileNotFoundError(f\"No .hdr files found in {hdr_folder}\")\n",
    "\n",
    "# Ensure object and world exist\n",
    "obj = bpy.data.objects.get(object_name)\n",
    "if not obj:\n",
    "    raise ValueError(f\"Object '{object_name}' not found\")\n",
    "\n",
    "if not obj.data.materials:\n",
    "    raise ValueError(f\"Object '{object_name}' has no materials\")\n",
    "\n",
    "material = obj.data.materials[0]\n",
    "if not material.use_nodes:\n",
    "    raise ValueError(f\"Material on '{object_name}' is not using nodes\")\n",
    "\n",
    "# Get image texture node for object\n",
    "obj_tex_node = material.node_tree.nodes.get(object_texture_node_name)\n",
    "if not obj_tex_node or obj_tex_node.type != \"TEX_IMAGE\":\n",
    "    raise ValueError(f\"Image texture node '{object_texture_node_name}' not found in object material\")\n",
    "\n",
    "# Get environment texture node in world\n",
    "world = bpy.context.scene.world\n",
    "if not world or not world.use_nodes:\n",
    "    raise ValueError(\"World material not found or not using nodes\")\n",
    "\n",
    "world_tex_node = world.node_tree.nodes.get(world_texture_node_name)\n",
    "if not world_tex_node or world_tex_node.type != \"TEX_ENVIRONMENT\":\n",
    "    raise ValueError(f\"Environment texture node '{world_texture_node_name}' not found in world material\")\n",
    "# --------------------------------\n",
    "#endregion\n",
    "\n",
    "# -------- UTILITY FUNCTIONS --------\n",
    "def convert_old_value_to_new_range(newMin, newMax, oldValue):\n",
    "    oldRange = (1 - 0)\n",
    "    newRange = (newMax - newMin)\n",
    "    newValue = (((oldValue - 0) * newRange) / oldRange) + newMin\n",
    "    return newValue\n",
    "# --------------------------------\n",
    "\n",
    "\n",
    "# -------- FRAME UPDATE --------\n",
    "# Handler function to change background texture\n",
    "def randomize_environment_texture(scene):\n",
    "    frame = scene.frame_current\n",
    "    if frame % change_interval != 0:\n",
    "        return  # only change on frames divisible by 10\n",
    "\n",
    "    index = (frame // change_interval) % len(hdr_files)\n",
    "    hdr_path = os.path.join(hdr_folder, hdr_files[index])\n",
    "\n",
    "    print(f\"Frame {frame}: Loading {hdr_path}\")\n",
    "\n",
    "    # Load or reuse image\n",
    "    img = bpy.data.images.get(hdr_files[index])\n",
    "    if not img:\n",
    "        img = bpy.data.images.load(hdr_path)\n",
    "\n",
    "    # Update object image texture\n",
    "    obj_tex_node.image = img\n",
    "\n",
    "    # Update world environment texture\n",
    "    world_tex_node.image = img\n",
    "\n",
    "# Handler function to change camera position and rotation\n",
    "def randomize_camera_transform(scene):\n",
    "    current_frame = scene.frame_current\n",
    "    random.seed(current_frame)  # Seed random number generator for reproducibility\n",
    "\n",
    "    # Get scene objects\n",
    "    target_obj = bpy.data.objects[target_object_name]\n",
    "    camera_obj = bpy.data.objects[camera_name]\n",
    "    \n",
    "    # Generate random position\n",
    "    random_radius = random.random()\n",
    "    random_theta = random.random()\n",
    "    random_phi = random.random()\n",
    "\n",
    "    random_radius = convert_old_value_to_new_range(radius_range[0], radius_range[1], random_radius)\n",
    "    random_theta = convert_old_value_to_new_range(theta_range[0], theta_range[1], random_theta)\n",
    "    random_phi = convert_old_value_to_new_range(phi_range[0], phi_range[1], random_phi)\n",
    "\n",
    "    # Generate random rotation offsets\n",
    "    random_rotation_x = random.random()\n",
    "    ransom_rotation_z = random.random()\n",
    "    \n",
    "    # Camera position in world space\n",
    "    x_position = random_radius * math.sin(random_phi) * math.cos(random_theta)\n",
    "    y_position = random_radius * math.sin(random_phi) * math.sin(random_theta)\n",
    "    z_position = random_radius * math.cos(random_phi)\n",
    "\n",
    "    # Set camera location\n",
    "    camera_obj.location = mathutils.Vector((x_position, y_position, z_position))\n",
    "\n",
    "    # First point camera at target and them apply the random offsets\n",
    "    # Get direction vector from camera to target\n",
    "    direction = target_obj.location - camera_obj.location\n",
    "    direction.normalize()\n",
    "\n",
    "    # Create a rotation matrix that points the camera's -Z axis toward the target (Blender's camera looks down -Z)\n",
    "    up = mathutils.Vector((0.0, 0.0, 1.0))  # world up axis\n",
    "    rotation = direction.to_track_quat('-Z', 'Y').to_euler()\n",
    "\n",
    "    # Apply the rotation to the camera directly\n",
    "    camera_obj.rotation_euler = rotation\n",
    "\n",
    "    # Add random camera offsets AFTER the camera is pointing at the target\n",
    "    camera_obj.rotation_euler[0] += convert_old_value_to_new_range(x_rotation_offset_range[0], x_rotation_offset_range[1], random_rotation_x)\n",
    "    camera_obj.rotation_euler[1] = 0\n",
    "    camera_obj.rotation_euler[2] += convert_old_value_to_new_range(z_rotation_offset_range[0], z_rotation_offset_range[1], ransom_rotation_z)\n",
    "# --------------------------------\n",
    "\n",
    "# -------- SET UP FRAME UPDATES --------\n",
    "bpy.app.handlers.frame_change_pre.clear()\n",
    "bpy.app.handlers.frame_change_pre.append(randomize_camera_transform)\n",
    "bpy.app.handlers.frame_change_pre.append(randomize_environment_texture)\n",
    "# --------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generating the Image data, go to render properties and change the `max samples` under `Sampling` → `Render` to 1, and under `Film` check the `Transparent` option\n",
    "\n",
    "Then change both materials of the Coral Object to the `RED_Mat` material, then delete the `Sphere` object\n",
    "\n",
    "Finally, select a different output folder and render the animation to get the masks for your Image data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Converting Images to Grayscale\n",
    "To force the model to rely less on color data, we can convert the rendered RGB images to grayscale using OpenCV or similar libraries.\n",
    "\n",
    "Use the cell below to perform the conversion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder containing image data from Blender renders\n",
    "image_data_folder = \"Path/To/Image/Data\"\n",
    "\n",
    "# Output folder for grayscale images\n",
    "output_folder = \"Output/Path/For/GrayScale/Images\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "os.makedirs(os.path.join(output_folder, \"Train\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_folder, \"Val\"), exist_ok=True)\n",
    "\n",
    "# Convert all images in the image data folder to grayscale and save to output folder, train, and val subfolders\n",
    "image_files = [f for f in os.listdir(image_data_folder) if f.lower().endswith('.jpg')]\n",
    "image_files.sort()  # for reproducibility\n",
    "\n",
    "num_val = max(1, int(0.1 * len(image_files)))\n",
    "val_files = image_files[:num_val]\n",
    "train_files = image_files[num_val:]\n",
    "\n",
    "for fname in train_files:\n",
    "    img = cv2.imread(os.path.join(image_data_folder, fname))\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    cv2.imwrite(os.path.join(output_folder, \"Train\", fname), gray)\n",
    "\n",
    "for fname in val_files:\n",
    "    img = cv2.imread(os.path.join(image_data_folder, fname))\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    cv2.imwrite(os.path.join(output_folder, \"Val\", fname), gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generating Annotations in YOLOv11 Format\n",
    "Once images are rendered, we need to generate bounding box annotations in YOLOv11 format. This format includes class ID and normalized coordinates for each object.\n",
    "\n",
    "Use the cell below to generate annotation files.\n",
    "\n",
    "In this example we generate annotation files for YOLOv11 [Object Detection](https://docs.ultralytics.com/tasks/detect/), with some small modifications you can also generate annotations for [Oriented Bounding Boxes Object Detection](https://docs.ultralytics.com/tasks/obb/) and [Instance Segmentation](https://docs.ultralytics.com/tasks/segment/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of the grayscale images\n",
    "gray_scale_images_folder_train = os.path.join(output_folder, \"Train\")\n",
    "gray_scale_images_folder_val = os.path.join(output_folder, \"Val\")\n",
    "\n",
    "# Path of the mask images from Blender\n",
    "mask_images_folder = \"Path/To/Mask/Images\"\n",
    "\n",
    "# Get train/val split from grayscale images\n",
    "train_files = set(os.listdir(gray_scale_images_folder_train))\n",
    "val_files = set(os.listdir(gray_scale_images_folder_val))\n",
    "\n",
    "# Define range of red color in HSV\n",
    "lower_red = np.array([0, 50, 50])\n",
    "upper_red = np.array([10, 255, 255])\n",
    "\n",
    "for filename in os.listdir(mask_images_folder):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        img = cv2.imread(os.path.join(mask_images_folder, filename))\n",
    "\n",
    "        # Convert BGR to HSV\n",
    "        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        # Threshold the HSV image to get only red colors\n",
    "        mask = cv2.inRange(hsv, lower_red, upper_red)\n",
    "\n",
    "        # Find contours in the binary image\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        if contours:\n",
    "            # Find the bounding box of the largest contour\n",
    "            x, y, w, h = cv2.boundingRect(max(contours, key=cv2.contourArea))\n",
    "\n",
    "            # Calculate normalized bounding box coordinates\n",
    "            x_center = (x + w/2) / img.shape[1]\n",
    "            y_center = (y + h/2) / img.shape[0]\n",
    "            width = w / img.shape[1]\n",
    "            height = h / img.shape[0]\n",
    "\n",
    "            # Determine output folder (train or val)\n",
    "            if filename in train_files:\n",
    "                txt_folder = gray_scale_images_folder_train\n",
    "            elif filename in val_files:\n",
    "                txt_folder = gray_scale_images_folder_val\n",
    "            else:\n",
    "                continue  # skip if not found in either set\n",
    "\n",
    "            # Save bounding box coordinates in YOLOv8 format\n",
    "            with open(os.path.join(txt_folder, filename.replace(\".jpg\", \".txt\")), 'w') as f:\n",
    "                f.write(f'0 {x_center} {y_center} {width} {height}')\n",
    "\n",
    "\n",
    "# Create YOLOv11 YAML configuration file\n",
    "yaml_content = f\"\"\"\n",
    "train: {gray_scale_images_folder_train}\n",
    "val: {gray_scale_images_folder_val}\n",
    "nc: 1\n",
    "names: ['coral']\n",
    "\"\"\"\n",
    "\n",
    "yaml_path = os.path.join(output_folder, \"yolov11_config.yaml\")\n",
    "with open(yaml_path, 'w') as f:\n",
    "    f.write(yaml_content)\n",
    "\n",
    "print(f\"YOLOv11 configuration file saved to {yaml_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training YOLOv11 Object Detection Model\n",
    "With the dataset and annotations ready, we can now train a YOLOv11 object detection model. This step involves configuring the model, loading the dataset, and running the training process.\n",
    "\n",
    "Use the cell below to initiate training.\n",
    "\n",
    "For more info visit the [ultralytics website](https://docs.ultralytics.com/tasks/detect/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model\n",
    "model = YOLO(\"yolo11n.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Train the model\n",
    "results = model.train(data=yaml_path, epochs=100, imgsz=640, device=0)  # device=0 for GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Using the Trained Model for Detection\n",
    "After training, we can use the model to detect objects in new images. This step involves loading the trained weights and running predictions on test images.\n",
    "\n",
    "Use the cell below to perform prediction on test images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your test image path\n",
    "test_image = cv2.imread(\"Path/To/Test/Image.jpg\")\n",
    "\n",
    "# Replace with your best.pt path\n",
    "model = YOLO(\"runs/detect/train1/weights/best.pt\")\n",
    "\n",
    "# Test model\n",
    "result = model.predict(source=test_image, save=True, show=True, conf=0.5, save_txt=False)\n",
    "\n",
    "# Display results\n",
    "result[0].show()\n",
    "print(result[0].boxes.xywh)  # x_center, y_center, width, height"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
